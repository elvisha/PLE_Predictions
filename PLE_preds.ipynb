{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9203fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, r2_score, classification_report\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, GroupKFold, GroupShuffleSplit, StratifiedKFold\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.utils import shuffle\n",
    "from statsmodels.stats.multitest import multipletests as fdr\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and clean up ABCD data\n",
    "# set base dirctories\n",
    "ABCD_base_dir   = '/Users/elvishadhamala/Documents/yale/ABCD'\n",
    "\n",
    "#load subj fc data\n",
    "ABCD_fc= pd.read_csv(os.path.join(ABCD_base_dir, 'ABCD_rsfc_withsubcort_5260subj.csv'), header=None)\n",
    "ABCD_fc = ABCD_fc.T\n",
    "ABCD_subj = pd.read_csv(os.path.join(ABCD_base_dir, 'subjects_all_rs_all_score_unrelated_5260.txt'), header=None)\n",
    "ABCD_fc.insert(0, \"subjectkey\", ABCD_subj[0], True)\n",
    "ABCD_fc = ABCD_fc.sort_values(by='subjectkey', ascending=True)\n",
    "\n",
    "# load subj sex/gender data and site data\n",
    "ABCD_psychosis = pd.read_csv(os.path.join(ABCD_base_dir, 'abcd_mhy02.csv'), header=0)\n",
    "ABCD_site = pd.read_csv(os.path.join(ABCD_base_dir, 'abcd_lt01.csv'), header=0)\n",
    "ABCD_motion = pd.read_csv(os.path.join(ABCD_base_dir, 'score_motion_all_subjects_mf.csv'))\n",
    "\n",
    "#drop duplicate header rows\n",
    "header_row = 0\n",
    "ABCD_psychosis = ABCD_psychosis.drop(header_row)\n",
    "ABCD_site = ABCD_site.drop(header_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get site data for relevant subjects\n",
    "ABCD_site = ABCD_site[ABCD_site.subjectkey.isin(ABCD_fc.subjectkey)]\n",
    "ABCD_site_y1 = ABCD_site[ABCD_site.eventname == 'baseline_year_1_arm_1']\n",
    "ABCD_site_y1 = ABCD_site_y1.sort_values(by='subjectkey', ascending=True)\n",
    "ABCD_site_y1 = ABCD_site_y1[['subjectkey', 'site_id_l']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select subjects for whom we have FC data\n",
    "ABCD_psychosis = ABCD_psychosis[ABCD_psychosis.subjectkey.isin(ABCD_fc.subjectkey)]\n",
    "ABCD_psychosis_y1 = ABCD_psychosis[ABCD_psychosis.eventname == 'baseline_year_1_arm_1']\n",
    "ABCD_psychosis_y1 = ABCD_psychosis_y1.sort_values(by='subjectkey', ascending=True)\n",
    "ABCD_psychosis_y1 = ABCD_psychosis_y1[['subjectkey', 'sex', 'eventname',\n",
    "                                       'pps_y_ss_number', 'pps_y_ss_severity_score']]\n",
    "ABCD_psychosis_y1_sum = ABCD_psychosis_y1.dropna(how='all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc8e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean fc and site data to match gender data\n",
    "ABCD_fc_clean = ABCD_fc[ABCD_fc.subjectkey.isin(ABCD_psychosis_y1_sum.subjectkey)]\n",
    "ABCD_site_clean = ABCD_site_y1[ABCD_site_y1.subjectkey.isin(ABCD_psychosis_y1_sum.subjectkey)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658628e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort data by subject key\n",
    "ABCD_fc_clean = ABCD_fc_clean.sort_values(by='subjectkey', ascending=True)\n",
    "ABCD_site_clean = ABCD_site_clean.sort_values(by='subjectkey', ascending=True)\n",
    "ABCD_psychosis_y1_sum = ABCD_psychosis_y1_sum.sort_values(by='subjectkey', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in motion data for the participants\n",
    "mask = ABCD_motion.subjectkey.isin(ABCD_fc_clean['subjectkey'])\n",
    "ABCD_motion_incl = ABCD_motion[mask]\n",
    "ABCD_motion_data = ABCD_motion_incl.sort_values(by='subjectkey', ascending=True)\n",
    "ABCD_motion_data.reset_index(inplace=True) \n",
    "ABCD_motion_data = ABCD_motion_data.drop(columns=['index'])\n",
    "ABCD_motion_m = ABCD_motion_data[ABCD_motion_data.gender=='M']\n",
    "ABCD_motion_f = ABCD_motion_data[ABCD_motion_data.gender=='F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned fc, gender, and site data\n",
    "ABCD_fc_data = ABCD_fc_clean.drop(columns=['subjectkey'])\n",
    "ABCD_psychosis_data = ABCD_psychosis_y1_sum.drop(columns=['subjectkey'])\n",
    "ABCD_site_data = ABCD_site_clean.drop(columns=['subjectkey'])\n",
    "\n",
    "#clean subj data\n",
    "ABCD_fc_data.reset_index(inplace=True) \n",
    "ABCD_psychosis_data.reset_index(inplace=True)\n",
    "ABCD_site_data.reset_index(inplace=True)\n",
    "\n",
    "ABCD_fc_data = ABCD_fc_data.drop(columns=['index'])\n",
    "ABCD_psychosis_data = ABCD_psychosis_data.drop(columns=['index'])\n",
    "ABCD_site_data = ABCD_site_data.drop(columns=['index'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sex-specific variables\n",
    "mask_m = ABCD_psychosis_data.sex=='M'\n",
    "ABCD_fc_m = ABCD_fc_data[mask_m]\n",
    "ABCD_psychosis_m = np.double(ABCD_psychosis_data[mask_m].pps_y_ss_severity_score)\n",
    "ABCD_sex_m = ABCD_psychosis_data[mask_m].sex\n",
    "ABCD_site_m = ABCD_site_data[mask_m].site_id_l\n",
    "\n",
    "#get sex-specific variables\n",
    "mask_f = ABCD_psychosis_data.sex=='F'\n",
    "ABCD_fc_f = ABCD_fc_data[mask_f]\n",
    "ABCD_psychosis_f = np.double(ABCD_psychosis_data[mask_f].pps_y_ss_severity_score)\n",
    "ABCD_sex_f = ABCD_psychosis_data[mask_f].sex\n",
    "ABCD_site_f = ABCD_site_data[mask_f].site_id_l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2056e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate values so it's easier to combine and put into the model all together \n",
    "#fc = np.concatenate([ABCD_fc_m, ABCD_fc_f])\n",
    "#psychosis = np.concatenate([ABCD_psychosis_m, ABCD_psychosis_f])\n",
    "#sex = np.concatenate([ABCD_sex_m, ABCD_sex_f])\n",
    "#site = np.concatenate([ABCD_site_m, ABCD_site_f])\n",
    "\n",
    "fc = np.concatenate([ABCD_fc_m])\n",
    "psychosis = np.concatenate([ABCD_psychosis_m])\n",
    "sex = np.concatenate([ABCD_sex_m])\n",
    "site = np.concatenate([ABCD_site_m])\n",
    "\n",
    "\n",
    "sex[sex=='M']=1\n",
    "sex[sex=='F']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude any remaining nan\n",
    "nanmask = np.isnan(psychosis)\n",
    "psychosis = psychosis[~nanmask]\n",
    "sex = sex[~nanmask]\n",
    "sex = np.double(sex)\n",
    "fc = fc[~nanmask]\n",
    "site = site[~nanmask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '/Users/elvishadhamala/Documents/BPM/psychosis_preds/'\n",
    "\n",
    "#number of repetitions you want to perform\n",
    "rep = 100\n",
    "#number of folds you want in the cross-validation\n",
    "k = 3\n",
    "#proportion of data you want in your training set and test set\n",
    "train_size = .66\n",
    "test_size = 1-train_size\n",
    "\n",
    "#regression model type\n",
    "regr = Ridge(max_iter=1000000, solver='lsqr')\n",
    "#set hyperparameter grid space you want to search through for the model\n",
    "#adapted from the Thomas Yeo Lab Github: \n",
    "#ThomasYeoLab/CBIG/blob/master/stable_projects/predict_phenotypes/He2019_KRDNN/KR_HCP/CBIG_KRDNN_KRR_HCP.m\n",
    "#alphas = [0, 0.00001, 0.0001, 0.001, 0.004, 0.007, 0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1, 1.5, 2, 2.5, 3,\n",
    "#          3.5, 4, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 100, 150, 200, 300, 500, 700, 1000, 2000, \n",
    "#          3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 2, 3, 4, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 100, 150, 200, 300, 500, 700, 1000]\n",
    "\n",
    "#alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "#param grid set to the hyperparamters you want to search through\n",
    "paramGrid ={'alpha': alphas}\n",
    "\n",
    "#set x data to be the input variable you want to use\n",
    "X = fc\n",
    "Y = psychosis.T\n",
    "\n",
    "#number of features \n",
    "n_feat = X.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa578a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r^2 - coefficient of determination\n",
    "r2 = np.zeros([rep])\n",
    "#explained variance\n",
    "var = np.zeros([rep])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr = np.zeros([rep])\n",
    "#optimised alpha (hyperparameter)\n",
    "opt_alpha = np.zeros([rep])\n",
    "#feature importance extracted from the model\n",
    "featimp = np.zeros([rep,n_feat])\n",
    "#for when the feat weights get haufe-inverted\n",
    "featimp_haufe = np.zeros([rep,n_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56184f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ABCD_fc\n",
    "del ABCD_fc_clean\n",
    "\n",
    "\n",
    "del ABCD_psychosis\n",
    "del ABCD_psychosis_y1\n",
    "del ABCD_psychosis_y1_sum\n",
    "\n",
    "del ABCD_site\n",
    "del ABCD_site_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db13ed21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in range(rep):\n",
    "    \n",
    "    #print model # you're on\n",
    "    print('Model %d' %(p+1))\n",
    "    \n",
    "    #if running null models need to shuffle the Y values within site\n",
    "    #values = Y\n",
    "    #groups = site\n",
    "\n",
    "    #for index in np.unique(groups):\n",
    "    #    mask = groups==index\n",
    "    #    values[mask] = np.random.permutation(values[mask])\n",
    "    #Y_shuffle = values\n",
    "    \n",
    "    #split into train/test data\n",
    "    train_inds, test_inds = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state = p).split(X, groups=site))\n",
    "    \n",
    "    #set x values based on indices from split\n",
    "    x_train = X[train_inds]\n",
    "    x_test = X[test_inds]\n",
    "        \n",
    "    #set y values based on indices from split  \n",
    "    beh_train = Y[train_inds]\n",
    "    beh_test = Y[test_inds]\n",
    "    \n",
    "    #set y values based on indices from split for null predictions  \n",
    "    #beh_train = Y_shuffle[train_inds]\n",
    "    #beh_test = Y_shuffle[test_inds]\n",
    "    \n",
    "    #set site data based on indices from split\n",
    "    site_train = site[train_inds]\n",
    "    site_test = site[test_inds] \n",
    "    \n",
    "    #convert y values to to double\n",
    "    y_train = np.double(beh_train)\n",
    "    y_test = np.double(beh_test)\n",
    "\n",
    "    #create variables to store nested CV scores, and best parameters from hyperparameter optimisation\n",
    "    best_scores = []\n",
    "    best_params = []\n",
    "    \n",
    "    #set parameters for inner and outer loops for CV\n",
    "    cv_split = GroupKFold(n_splits=k)\n",
    "        \n",
    "    print (\"Optimising Models\")\n",
    "            \n",
    "    #define regressor with grid-search CV for inner loop\n",
    "    gridSearch = GridSearchCV(estimator=regr, param_grid=paramGrid, n_jobs=-1, verbose=0, cv=cv_split, scoring='explained_variance')\n",
    "\n",
    "    #fit regressor to the model, use site ID as group category again\n",
    "    gridSearch.fit(x_train, y_train, groups=site_train)\n",
    "\n",
    "    #save parameters corresponding to the best score\n",
    "    best_params.append(list(gridSearch.best_params_.values()))\n",
    "    best_scores.append(gridSearch.best_score_)\n",
    "        \n",
    "    print (\"Evaluating Models\")\n",
    "        \n",
    "    #save optimised alpha values\n",
    "    opt_alpha[p] = best_params[best_scores.index(np.max(best_scores))][0]\n",
    "\n",
    "    #for null prediction - use random alpha from set of optimized alphas \n",
    "    #rand_alpha = np.random.choice(alphas)\n",
    "    \n",
    "    #fit optimized models\n",
    "    model = Ridge(alpha = opt_alpha[p], max_iter=1000000, solver='lsqr')\n",
    "    #model = Ridge(alpha = rand_alpha, max_iter=1000000, solver='lsqr')\n",
    "\n",
    "    model.fit(x_train, y_train);\n",
    "        \n",
    "    #evaluate model within sex within behavior\n",
    "    r2[p]=model.score(x_test,y_test)\n",
    "    p#rint(r2[p])\n",
    "        \n",
    "    preds = []\n",
    "    preds = model.predict(x_test).ravel()\n",
    "    \n",
    "    #compute explained variance \n",
    "    var[p] = explained_variance_score(y_test, preds)\n",
    "    #print(var[p])\n",
    "\n",
    "    #compute correlation between true and predicted (prediction accuracy)\n",
    "    corr[p] = np.corrcoef(y_test.ravel(), preds)[1,0]\n",
    "    #print(corr[p])\n",
    "    \n",
    "    print (\"Haufe-Transforming Feature Weights\")\n",
    "    \n",
    "    #extract feature importance\n",
    "    featimp[p,:] = model.coef_\n",
    "    \n",
    "    cov_x = []\n",
    "    cov_y_pred = []\n",
    "    \n",
    "    #generate predictions for y_train \n",
    "    y_train_preds = model.predict(x_train).ravel()\n",
    "    cov_y_pred = np.cov(y_train_preds)\n",
    "    \n",
    "\n",
    "    #compute Haufe-inverted feature weights\n",
    "    cov_x = np.cov(np.transpose(x_train))\n",
    "    \n",
    "    #perform matrix multiplication of cov x, feat weights, and divide by cov of predicted y_train\n",
    "    featimp_haufe[p,:] = featimp_haufe[p,:]*(cov_x)/cov_y_pred\n",
    "\n",
    "    #save all outputs\n",
    "    np.save((results_dir + 'fc_r2_sev_m.npy'),r2)\n",
    "    np.save((results_dir + 'fc_var_sev_m.npy'),var)\n",
    "    np.save((results_dir + 'fc_corr_sev_m.npy'),corr)\n",
    "    np.save((results_dir + 'fc_alpha_sev_m.npy'),opt_alpha)\n",
    "    np.save((results_dir + 'fc_featimp_sev_m.npy'),featimp)\n",
    "    np.save((results_dir + 'fc_featimp_haufe_sev_m.npy'),featimp_haufe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
